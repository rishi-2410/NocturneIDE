import os
from llama_cpp import Llama
from deep_translator import GoogleTranslator  # pip install deep-translator

# –ê–±—Å–æ–ª—é—Ç–Ω–∏–π —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª—ñ (.gguf)
current_dir = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(current_dir, 'models', 'falcon-7b-instruct.Q4_0.gguf')

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ llama-cpp-python
llm = Llama(model_path=model_path, n_ctx=2048)

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –ø–µ—Ä–µ–∫–ª–∞–¥–∞—á
translator = GoogleTranslator()

def translate(text, src_lang, dest_lang):
    try:
        return GoogleTranslator(source=src_lang, target=dest_lang).translate(text)
    except Exception as e:
        return f"Translation Error: {str(e)}"

def ask_ai(prompt):
    try:
        # üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –∑–∞–ø–∏—Ç –Ω–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É
        translated_prompt = translate(prompt, 'uk', 'en')

        # ‚ö° –î–æ–¥–∞—î–º–æ —Å–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º–ø—Ç
        system_prompt = (
            "You are a helpful AI assistant. Provide clear answers and code examples if needed. "
            "Respond in the user's language when possible."
        )
        full_prompt = f"{system_prompt}\n\nUser: {translated_prompt}\nAI:"

        # ‚ö° –í–∏–∫–ª–∏–∫ AI
        response = llm(full_prompt, max_tokens=100, temperature=0.3, top_p=0.7, stop=["User:", "AI:"])

        # üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞–∑–∞–¥ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É
        translated_response = translate(response["choices"][0]["text"], 'en', 'uk')

        return translated_response.strip()

    except Exception as e:
        return f"Error: {str(e)}"

# üü¢ –¢–µ—Å—Ç
if __name__ == "__main__":
    user_input = input("–í–≤–µ–¥—ñ—Ç—å –∑–∞–ø–∏—Ç: ")
    print("AI:", ask_ai(user_input))
